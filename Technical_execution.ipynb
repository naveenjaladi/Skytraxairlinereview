{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skytrax Airline Review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains airline review collected by Skytrax airline which is  United Kingdom-based consultancy which runs an airline and airport review and ranking site. \n",
    "\n",
    "* Dataset Link: https://www.kaggle.com/efehandanisman/skytrax-airline-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive analysis is performed to analyze the data. The dataset contains feilds like Airline, Overall, Author, Review_date, Customer_review, Aircraft, Traveller_type, Cabin, Route, Date_flown, Seat_Comfort, Cabin_service, Food_bev, Entertainment, Ground_service, Value_for_money, Recommended. As the features comprise of numerical data Linear Regression is used to predict the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing Spark building Spark session and then Loading the data into data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------------------+---------------+--------------------+--------------------+--------------+--------------+--------------------+-------------+------------+-------------+--------+-------------+--------------+---------------+-----------+\n",
      "|         airline|overall|              author|    review_date|     customer_review|            aircraft|traveller_type|         cabin|               route|   date_flown|seat_comfort|cabin_service|food_bev|entertainment|ground_service|value_for_money|recommended|\n",
      "+----------------+-------+--------------------+---------------+--------------------+--------------------+--------------+--------------+--------------------+-------------+------------+-------------+--------+-------------+--------------+---------------+-----------+\n",
      "|Turkish Airlines|     10|         Zeshan Shah|   6th May 2019|Ã¢Å“â€¦ Trip Veri...|                A330|  Solo Leisure| Economy Class|Washington Dulles...|   April 2019|           4|            5|       5|            5|             5|              5|          1|\n",
      "|Turkish Airlines|      2|            S Gonser|29th April 2019|Ã¢Å“â€¦ Trip Veri...|Boeing 737-800 / ...|  Solo Leisure| Economy Class|Basel to Cape Tow...|   April 2019|           3|            3|       2|            3|             1|              2|          0|\n",
      "|Turkish Airlines|      6|          Sami Osman|29th April 2019|Not Verified | Ab...|   A320 / Boeing 737|  Solo Leisure| Economy Class|Abu Dhabi to Luxe...|   April 2019|           2|            3|       3|            3|             3|              3|          1|\n",
      "|Turkish Airlines|      1|Norka Idalia Orlando|28th April 2019|Ã¢Å“â€¦ Trip Veri...|         A320 / A330|  Solo Leisure| Economy Class|Venice to Boston ...|February 2019|           1|            1|       1|            1|             1|              1|          0|\n",
      "|Turkish Airlines|      2|      Trevor Khurana|24th April 2019|Ã¢Å“â€¦ Trip Veri...|      Boeing 777-300|  Solo Leisure| Economy Class|Houston to Kiev v...|   March 2019|           1|            3|       2|            2|             1|              1|          0|\n",
      "|Turkish Airlines|      5|       Gyan Fernando|21st April 2019|Ã¢Å“â€¦ Trip Veri...|      Boeing 737-800|  Solo Leisure| Economy Class|Nakhchivan to Ist...|   April 2019|           2|            2|       3|            1|             2|              3|          1|\n",
      "|Turkish Airlines|      7|       Gyan Fernando|21st April 2019|Ã¢Å“â€¦ Trip Veri...|    Boeing 777-300ER|  Solo Leisure| Economy Class|London Heathrow t...|   April 2019|           4|            3|       3|            3|             4|              4|          1|\n",
      "|Turkish Airlines|      8|             S Tekin|20th April 2019|Ã¢Å“â€¦ Trip Veri...|          Boeing 777|  Solo Leisure| Economy Class|   Izmir to Istanbul|   April 2019|           4|            5|       4|            4|             5|              5|          1|\n",
      "|Turkish Airlines|     10|        P Bartinescu|15th April 2019|Ã¢Å“â€¦ Trip Veri...|                A330|Couple Leisure|Business Class|Montreal to Bucha...|   April 2019|           5|            5|       5|            5|             5|              5|          1|\n",
      "|Turkish Airlines|      9|             W Keale|11th April 2019|Ã¢Å“â€¦ Trip Veri...|                A330|  Solo Leisure| Economy Class|Istanbul to Manch...|   April 2019|           4|            5|       4|            4|             4|              5|          1|\n",
      "+----------------+-------+--------------------+---------------+--------------------+--------------------+--------------+--------------+--------------------+-------------+------------+-------------+--------+-------------+--------------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "#SparkSession is now the entry point of Spark \n",
    "#SparkSession can also be construed as gateway to spark libraries \n",
    "\n",
    "#creating instance of spark class \n",
    "spark=SparkSession.builder.appName('skytrax').getOrCreate() \n",
    "\n",
    "#creating spark dataframe from input csv file \n",
    "df=spark.read.csv(\"V:\\IOS\\Big Data Analytics\\Technical Final\\cleann_capstone_airline_reviews.csv\"\n",
    "\t\t\t\t,inferSchema=True,header=True) \n",
    "df.show(10) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now check for the Schema of the loaded dataframe as some columns Numeric Columns are stored in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      " |-- customer_review: string (nullable = true)\n",
      " |-- aircraft: string (nullable = true)\n",
      " |-- traveller_type: string (nullable = true)\n",
      " |-- cabin: string (nullable = true)\n",
      " |-- route: string (nullable = true)\n",
      " |-- date_flown: string (nullable = true)\n",
      " |-- seat_comfort: string (nullable = true)\n",
      " |-- cabin_service: string (nullable = true)\n",
      " |-- food_bev: string (nullable = true)\n",
      " |-- entertainment: string (nullable = true)\n",
      " |-- ground_service: string (nullable = true)\n",
      " |-- value_for_money: string (nullable = true)\n",
      " |-- recommended: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prints structure of dataframe along with datatype \n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converting the Seat_comfort, Cabin_service, Foob_bev, entertainment, Value_for_money, Ground_service, Recommended which are numerical but stored in a string format. \n",
    "* By Importing Pyspark Sql cast the required feilds into intergers from string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"seat_comfort\", df.seat_comfort.cast('integer'))\n",
    "df = df.withColumn(\"cabin_service\", df.cabin_service.cast('integer'))\n",
    "df = df.withColumn(\"food_bev\", df.food_bev.cast('integer'))\n",
    "df = df.withColumn(\"entertainment\", df.entertainment.cast('integer'))\n",
    "df = df.withColumn(\"value_for_money\", df.value_for_money.cast('integer'))\n",
    "df = df.withColumn(\"ground_service\", df.ground_service.cast('integer'))\n",
    "df = df.withColumn(\"recommended\", df.ground_service.cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check for the Schema for all columns in the dataframe the schema of casted columns are changed into interger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- overall: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      " |-- customer_review: string (nullable = true)\n",
      " |-- aircraft: string (nullable = true)\n",
      " |-- traveller_type: string (nullable = true)\n",
      " |-- cabin: string (nullable = true)\n",
      " |-- route: string (nullable = true)\n",
      " |-- date_flown: string (nullable = true)\n",
      " |-- seat_comfort: integer (nullable = true)\n",
      " |-- cabin_service: integer (nullable = true)\n",
      " |-- food_bev: integer (nullable = true)\n",
      " |-- entertainment: integer (nullable = true)\n",
      " |-- ground_service: integer (nullable = true)\n",
      " |-- value_for_money: integer (nullable = true)\n",
      " |-- recommended: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calling all the available columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline',\n",
       " 'overall',\n",
       " 'author',\n",
       " 'review_date',\n",
       " 'customer_review',\n",
       " 'aircraft',\n",
       " 'traveller_type',\n",
       " 'cabin',\n",
       " 'route',\n",
       " 'date_flown',\n",
       " 'seat_comfort',\n",
       " 'cabin_service',\n",
       " 'food_bev',\n",
       " 'entertainment',\n",
       " 'ground_service',\n",
       " 'value_for_money',\n",
       " 'recommended']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In our predictive model, below are the columns \n",
    "df.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature selection: The required Fetaures or the independent varibales have to be selected for the model.\n",
    "* Aircraft Feild is in String type But it is an feature required for Model. As the linear regression is used for the model. Aircraft feild which contains name of type of Aircraft Like boeing 777 has to be converted into Numeric datatype.\n",
    "* By using String indexer the value is can be changed into numreic value and new columns is added to the data frame with the column name 'aircraft_cat' which contains Numeric datatype of aircraft.\n",
    "* And then displaying the first five rows to check for the new column of aircraft_cat is added to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(airline='Turkish Airlines', overall=10, author='Zeshan Shah', review_date='6th May 2019', customer_review='Ã¢Å“â€¦ Trip Verified | Flew on Turkish Airlines IAD-IST-KHI and return KHI-IST-IAD. Turkish Airlines has consistently maintained its quality since I first flew with them in 2007. The flights leave on time, the catering is excellent, the inflight entertainment is extensive and the interface easy to use, and the cabin crew is excellent. Interesting though the A330 on the KHI-IST route and return seemed to have more leg room and was newer than the A330 on the IAD-IST route which was showing its age. The A330 on the IAD-IST route had a slow responding interface for the inflight entertainment and a broken table on the return flight. But Turkish Airlines will be replacing the A330 on its flight to IAD with the 787 sometime in the summer. Turkish food was served on the return leg which I personally like, and I saw the cabin staff helping elderly passengers walk to the lavatory which was nice. Overall another wonderful experience with Turkish Airlines.', aircraft='A330', traveller_type='Solo Leisure', cabin='Economy Class', route='Washington Dulles to Karachi', date_flown='April 2019', seat_comfort=4, cabin_service=5, food_bev=5, entertainment=5, ground_service=5, value_for_money=5, recommended=5, aircraft_cat=3.0)\n",
      "\n",
      "\n",
      "Row(airline='Turkish Airlines', overall=2, author='S Gonser', review_date='29th April 2019', customer_review='Ã¢Å“â€¦ Trip Verified | Basel to Cape Town via Istanbul. When I arrived in Istanbul at 10pm we are informed that TK44 to Cape Town (departure at 1.55am) is delayed 4,5 hours. No reason for this Delay was announced. I received a voucher for Burger King, Sbarro Pizza or Popeys. The staff at the gate were very unfriendly.', aircraft='Boeing 737-800 / A330-300', traveller_type='Solo Leisure', cabin='Economy Class', route='Basel to Cape Town via Istanbul', date_flown='April 2019', seat_comfort=3, cabin_service=3, food_bev=2, entertainment=3, ground_service=1, value_for_money=2, recommended=1, aircraft_cat=155.0)\n",
      "\n",
      "\n",
      "Row(airline='Turkish Airlines', overall=6, author='Sami Osman', review_date='29th April 2019', customer_review='Not Verified | Abu Dhabi to Luxembourg via Istanbul. From AUH-IST, as the flight was at 1:30am, the flight was I comfortable due to the small size of the aircraft. The seats were cramped although the leg room was okay-ish. Service started mid way through and the food was okay but nothing great. Staff was mostly friendly. IST-LUX staff was friendlier, aircraft was bit better, the food was of the same standard though. I think it was a decent experience overall but I would prefer a more spacious air craft even if the flight is 4.5 hours from AUH.', aircraft='A320 / Boeing 737', traveller_type='Solo Leisure', cabin='Economy Class', route='Abu Dhabi to Luxembourg via Istanbul', date_flown='April 2019', seat_comfort=2, cabin_service=3, food_bev=3, entertainment=3, ground_service=3, value_for_money=3, recommended=3, aircraft_cat=122.0)\n",
      "\n",
      "\n",
      "Row(airline='Turkish Airlines', overall=1, author='Norka Idalia Orlando', review_date='28th April 2019', customer_review='Ã¢Å“â€¦ Trip Verified | The experience with Turkish Airlines has been devastating one. First paid $200 Euros per luggage, totaling $400 Euros for 2 luggage that end up lost between Turkish Airlines and JetBlue. My itinerary was from Venice to Istanbul to Boston and from there with Jetblue to San Juan, Puerto Rico. Once I made it to Boston I looked for my luggage and it never made it to BOS. I immediately try to make a reclamation to Turkish Airlines at BOS and was told that I had to wait until my final destination. Once I made it to my final destination I made my claim. Turkish Airlines will not cooperate with Jetblue and even if JB was really trying to help is not much they could do because TATurkish Airlines did not care for my luggage. It never made to Boston. Turkish Airlines have been unhelpful. In my case, even presenting evidence to both airlines, neither is able to find my lost claim luggage. It seems the claim is a form which after being submitted it get to be submitted evidence, dead word, as they do not even comply with the search of the lost not delivered luggage in hands of Turkish Airlines.', aircraft='A320 / A330', traveller_type='Solo Leisure', cabin='Economy Class', route='Venice to Boston via Istanbul', date_flown='February 2019', seat_comfort=1, cabin_service=1, food_bev=1, entertainment=1, ground_service=1, value_for_money=1, recommended=1, aircraft_cat=50.0)\n",
      "\n",
      "\n",
      "Row(airline='Turkish Airlines', overall=2, author='Trevor Khurana', review_date='24th April 2019', customer_review='Ã¢Å“â€¦ Trip Verified | Houston to Kiev via Istanbul. Fares seem competitive, but there is a catch in terms of layover time. Mine from Houston to Kiev Via Istanbul had a return leg layover of 14 hours and though TK website says that a layover of over a certain hours gets a complimentary hotel stay. But there is a fine print somewhere which they always get a disqualified citing some wired Ã¢â‚¬Å“rulesÃ¢â‚¬Â� that I, even as a frequent traveler cannot comprehend. Another notable difference in Turkey is that there are endless and multiple security checks, and one wonders if one is going through a war zone . Lines at security checks and passport control ( yes, if you need to stay at a hotel for a long layover, one needs to get an e-VISA just to get out, check into a hotel and get back to airport to catch the connecting flight). Ended up paying way more than if I had chosen other airlines like LH or SAS, which I will keep in mind next time', aircraft='Boeing 777-300', traveller_type='Solo Leisure', cabin='Economy Class', route='Houston to Kiev via Istanbul', date_flown='March 2019', seat_comfort=1, cabin_service=3, food_bev=2, entertainment=2, ground_service=1, value_for_money=1, recommended=1, aircraft_cat=10.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#columns identified as features are as below: \n",
    "#['seat_comfort', 'cabin_service', 'food_bev', 'entertainment', 'ground_service', 'aircraft'] \n",
    "#to work on the features, spark MLlib expects every value to be in numeric form \n",
    "#feature 'aircraft' is string datatype \n",
    "#using StringIndexer, string type will be typecast to numeric datatype \n",
    "#import library strinindexer for typecasting \n",
    "\n",
    "from pyspark.ml.feature import StringIndexer \n",
    "indexer=StringIndexer(inputCol='aircraft',outputCol='aircraft_cat') \n",
    "indexed=indexer.fit(df).transform(df) \n",
    "\n",
    "#above code will convert string to numeric feature and create a new dataframe \n",
    "#new dataframe contains a new feature 'aircraft_cat' and can be used further \n",
    "#feature aircraft_cat is now vectorized and can be used to fed to model \n",
    "for item in indexed.head(5): \n",
    "\tprint(item) \n",
    "\tprint('\\n') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Predicting Overall Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing vector and Vector assembler as the MLlib takes input in vector form. So the selected Features are converted into vector form using Vector assembler\n",
    "* Features are transformed into vectors and Overall Rating column is selected for predictions. Features and Overall columns are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|overall|\n",
      "+--------------------+-------+\n",
      "|[4.0,5.0,5.0,5.0,...|     10|\n",
      "|[3.0,3.0,2.0,3.0,...|      2|\n",
      "|[2.0,3.0,3.0,3.0,...|      6|\n",
      "|[1.0,1.0,1.0,1.0,...|      1|\n",
      "|[1.0,3.0,2.0,2.0,...|      2|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "#creating vectors from features \n",
    "#Apache MLlib takes input if vector form \n",
    "assembler=VectorAssembler(inputCols=[ 'seat_comfort', \n",
    "'cabin_service', \n",
    "'food_bev', \n",
    "'entertainment', \n",
    "'ground_service', \n",
    "'aircraft_cat'],outputCol='features') \n",
    "assembler.setHandleInvalid(\"skip\").transform(indexed).show\n",
    "output=assembler.transform(indexed) \n",
    "output.select('features','overall').show(5) \n",
    "#output as below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecting features and Label which is Overall into final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final data consist of features and label which is Overall. \n",
    "final_data=output.select('features','overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displaying the head of the Final_data. Each row contains Selected features in a vector and along with the Overall Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([4.0, 5.0, 5.0, 5.0, 5.0, 3.0]), overall=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly Split the dataset into Train and test data. This is should be done using Random split because If the data in dataset is sequential data then the efficiency and preditictions will be not accurate to the original values.\n",
    "* Then displaying number of rows in train and test data And then statistics of train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12703\n",
      "5465\n",
      "+-------+-----------------+\n",
      "|summary|          overall|\n",
      "+-------+-----------------+\n",
      "|  count|            12703|\n",
      "|   mean|6.468393292922932|\n",
      "| stddev|3.086240866867872|\n",
      "|    min|                1|\n",
      "|    max|               10|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#splitting data into train and test \n",
    "train_data ,test_data = final_data.randomSplit([0.7,0.3]) \n",
    "print(train_data.count())\n",
    "print(test_data.count())\n",
    "train_data.describe().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing Linear Regression and applying it to the Features and Overall label as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import LinearRegression library \n",
    "from pyspark.ml.regression import LinearRegression \n",
    "#creating an object of class LinearRegression\n",
    "#object takes features and label as input arguments \n",
    "model=LinearRegression(featuresCol='features',labelCol='overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fitting the model. The fit finds the best fit for the model.\n",
    "* Evaluting the train data which shows the metrics like R square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass train_data to train model \n",
    "trained_model=model.fit(train_data) \n",
    "#evaluating model trained for Rsquared error \n",
    "overall_results=trained_model.evaluate(train_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the R Square value. The R square value close to the is the best value and describe realtion of model with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared Error : 0.8084649321207266\n"
     ]
    }
   ],
   "source": [
    "# R-squared\n",
    "print('Rsquared Error :',overall_results.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing the model with the unseen data.\n",
    "* Displaying First 5 rows in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing Model on unlabeled data \n",
    "#create unlabeled data from test_data \n",
    "#testing model on unlabeled data \n",
    "unlabeled_data=test_data.select('features') \n",
    "unlabeled_data.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predicting the overall rating from the test_data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[1.0,1.0,1.0,1.0,...|0.5957534173976191|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5957896828190867|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5957896828190867|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5957896828190867|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5957896828190867|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5958984790834896|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5958984790834896|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960072753478929|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960435407693605|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960435407693605|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5960798061908281|\n",
      "|[1.0,1.0,1.0,1.0,...|0.5961160716122957|\n",
      "|[1.0,1.0,1.0,1.0,...| 0.596188602455231|\n",
      "|[1.0,1.0,1.0,1.0,...| 0.596188602455231|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=trained_model.transform(unlabeled_data) \n",
    "predictions.show()\n",
    "#below are the results of output from test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing Regression metrics and calculating the metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.8242054366437856\n",
      "RMSE = 1.3506314955026726\n"
     ]
    }
   ],
   "source": [
    "# Squared Error\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "print(\"MSE = %s\" % overall_results.meanSquaredError)\n",
    "print(\"RMSE = %s\" % overall_results.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.9693644370694899\n",
      "Explained variance = 7.6999274380400085\n"
     ]
    }
   ],
   "source": [
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % overall_results.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % overall_results.explainedVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above Metrics Results shows that model accuracy is very good and can be use for predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for predicting recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing vector and Vector assembler as the MLlib takes input in vector form. So the selected Features are converted into vector form using Vector assembler\n",
    "* Features are transformed into vectors and Recommended column is selected for predictions. Features and Overall columns are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, recommended: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "#creating vectors from features \n",
    "#Apache MLlib takes input if vector form \n",
    "assembler=VectorAssembler(inputCols=[ 'seat_comfort', \n",
    "'cabin_service', \n",
    "'food_bev', \n",
    "'entertainment', \n",
    "'ground_service'],outputCol='features') \n",
    "assembler.setHandleInvalid(\"skip\").transform(df).show\n",
    "output=assembler.transform(df) \n",
    "output.select('features','recommended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selecting features and Label which is Overall into final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final data consist of features and label which is Recommended. \n",
    "final_data=output.select('features','recommended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly Split the dataset into Train and test data. This is should be done using Random split because If the data in dataset is sequential data then the efficiency and preditictions will be not accurate to the original values.\n",
    "* Then displaying number of rows in train and test data And then statistics of train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12737\n",
      "5431\n",
      "+-------+-----------------+\n",
      "|summary|      recommended|\n",
      "+-------+-----------------+\n",
      "|  count|            12737|\n",
      "|   mean| 3.44241187092722|\n",
      "| stddev|1.435320586993376|\n",
      "|    min|                1|\n",
      "|    max|                5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#splitting data into train and test \n",
    "train_data ,test_data = final_data.randomSplit([0.7,0.3]) \n",
    "print(train_data.count())\n",
    "print(test_data.count())\n",
    "train_data.describe().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing Linear Regression and applying it to the Features and Recommended label as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import LinearRegression library \n",
    "from pyspark.ml.regression import LinearRegression \n",
    "#creating an object of class LinearRegression \n",
    "#object takes features and label as input arguments \n",
    "model=LinearRegression(featuresCol='features',labelCol='recommended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fitting the model. The fit finds the best fit for the model.\n",
    "* Evaluting the train data which shows the metrics like R square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass train_data to train model \n",
    "trained_model=model.fit(train_data) \n",
    "#evaluating model trained for Rsquared error \n",
    "recommended_results=trained_model.evaluate(train_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the R Square value. The R square value close to the is the best value and describe realtion of model with the features.\n",
    "* The model has the R-sqaure value of 1 which shows the deep relation of features with the label and best model for predicting the label based on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared Error : 1.0\n"
     ]
    }
   ],
   "source": [
    "# R-squared\n",
    "print('Rsquared Error :',recommended_results.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing the model with the unseen data.\n",
    "* Displaying First 5 rows in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "|[1.0,1.0,1.0,1.0,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing Model on unlabeled data \n",
    "#create unlabeled data from test_data \n",
    "#testing model on unlabeled data \n",
    "unlabeled_data=test_data.select('features') \n",
    "unlabeled_data.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predicting the overall rating from the test_data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "|[1.0,1.0,1.0,1.0,...|0.9999999999999997|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions=trained_model.transform(unlabeled_data) \n",
    "predictions.show()\n",
    "#below are the results of output from test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing Regression metrics and calculating the metrics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.8242054366437856\n",
      "RMSE = 1.3506314955026726\n"
     ]
    }
   ],
   "source": [
    "# Squared Error\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "print(\"MSE = %s\" % overall_results.meanSquaredError)\n",
    "print(\"RMSE = %s\" % overall_results.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.9693644370694899\n",
      "Explained variance = 7.6999274380400085\n"
     ]
    }
   ],
   "source": [
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % overall_results.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % overall_results.explainedVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above Metrics Results shows that model accuracy is very good and can be use for predictive analysis.\n",
    "* The R square value of the model is 1 which the model is best model for the predictions and results best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Terminating the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
